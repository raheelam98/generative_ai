{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelam98/generative_ai/blob/main/RAG/pinecone/pinecone_vector_db.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Project : LangChain RAG with Google Gemini Flash and Pinecone**\n",
        "\n",
        "Creating a Retrieval-Augmented Generation (RAG) system using LangChain with Google Gemini Flash and Pinecone. This system will retrieve relevant context from a vector database and use that context to generate a more accurate and informed response from the Gemini model.\n",
        "\n"
      ],
      "metadata": {
        "id": "masu3dQcfeTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Implementation of RAG with Pinecone Vector DB**\n",
        "\n",
        "[Class-05: AI-201- Fundamentals of Agentic AI: Implementation of RAG with Pinecone Vector DB](https://www.youtube.com/watch?v=xQojOkqRbsU)\n",
        "\n",
        "Model : Google\n",
        "Vector DB : Pinecone\n",
        "\n",
        "#### **Tutorials**\n",
        "\n",
        "[LangChain Pinecone](https://python.langchain.com/docs/integrations/vectorstores/pinecone/)"
      ],
      "metadata": {
        "id": "PR5rqUXT8pN6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i1SN2iGf7RDk"
      },
      "outputs": [],
      "source": [
        "# Install the required packages:\n",
        "%%capture --no-stderr\n",
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Credentials**"
      ],
      "metadata": {
        "id": "nXXkJQfshFVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# save key in varaiable\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "# initialize pinecone\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n"
      ],
      "metadata": {
        "id": "wZOL6EphWdKI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zitvZtrAUGEI",
        "outputId": "1cc193a0-ccc2-48a7-fea9-fa37bac67a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pinecone.control.pinecone.Pinecone object at 0x7dfd3b719fd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Indexing**\n",
        "\n",
        "**Connect Vector Store to a Pinecone index**\n",
        "\n",
        "dimension is 768 becuse we are using google model\n",
        "\n",
        "Createing index through pinecone key"
      ],
      "metadata": {
        "id": "U0Ga0AMLhRnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Defining Index Name\n",
        "index_name=\"rag-pinecone-project-2\"\n",
        "\n",
        "# Creating a Pinecone Index\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n",
        "\n",
        "# Accessing the Index\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "V9UE8EsSUPcb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index)"
      ],
      "metadata": {
        "id": "p2374S7FOhuC",
        "outputId": "e6eea855-0791-4b53-af2b-4e525ebc4705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pinecone.data.index.Index object at 0x7e08ccbabb10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embedding**\n",
        "\n",
        "Use Google Gemini embeddings to vectorize documents.\n",
        "\n",
        "LangChain embeddings are vector representations of text used for semantic understanding and retrieval in AI workflows.\n",
        "\n",
        "Semantic refers to the meaning or interpretation of words, phrases, or symbols in a specific context.\n",
        "\n",
        "**Langchain Google Embeddings**\n",
        "\n",
        "[Google Generative AI Embeddings](https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai/)"
      ],
      "metadata": {
        "id": "Hn85wZDKTcH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
      ],
      "metadata": {
        "id": "rnaLaXY4V2nD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"Pinecone Vector DB!\")\n",
        "vector[:5]  # print last 5"
      ],
      "metadata": {
        "id": "T391sUARXM9i",
        "outputId": "b1d5d974-9fe0-40d6-8a3b-5fc5fe506246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.012879692018032074,\n",
              " -0.06462990492582321,\n",
              " -0.05366327241063118,\n",
              " -0.021048594266176224,\n",
              " 0.035343751311302185]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "w0XBIQpopTKb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Manage vector store**\n",
        "\n",
        "**Add items to vector store**\n",
        "\n",
        "Add items to our vector store by using the **`add_documents`** function."
      ],
      "metadata": {
        "id": "fVbQQCbxY31f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")"
      ],
      "metadata": {
        "id": "5IGc-pSMuz4a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(document_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt-csKtbvaCe",
        "outputId": "7d67473d-a879-4934-fb33-9054f1f98ebb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='I had chocalate chip pancakes and scrambled eggs for breakfast this morning.' metadata={'source': 'tweet'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n",
        "# uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "# vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "id": "yZ3546qJu5-l"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TMVvAkFwMNb",
        "outputId": "82123899-0190-4eb0-9d4c-44555f30415b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "uuid4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4PgJ_OdXAm7",
        "outputId": "5e58c697-0559-47e5-c806-c28bf5b5586d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UUID('58063efb-fe70-4f67-ab14-bdfd491e34e1')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add data to the vector database using  **`add_documents()`**\n",
        "\n",
        "**`from langchain_pinecone import PineconeVectorStore`**\n",
        "\n",
        "**`vector_store = PineconeVectorStore(index=index, embedding=embeddings)`**\n",
        "\n",
        "**`vector_store.add_documents(documents=documents, ids=uuids)`**\n"
      ],
      "metadata": {
        "id": "85u2ZGWZYA1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uuids = [str(uuid4()) for _ in range(len(documents))]"
      ],
      "metadata": {
        "id": "96oMy5Nwv8oS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pprlwRo7Xdo1",
        "outputId": "17b56f10-fa5e-418c-b559-c16a0eef38bc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['139c3175-2dcb-41db-bf54-c6a40b6d9068',\n",
              " '90f1ea7d-104e-4194-8652-3cdc0360abee',\n",
              " '3bb149e5-2572-45cf-aed5-1cd000f4cc44',\n",
              " 'be7c528b-63f0-4da2-a600-d604944f535e',\n",
              " 'a50e03d2-9859-4dd6-95f2-0257eb4fda7e',\n",
              " '8d1f908d-61ca-4485-b089-bd9e388943b2',\n",
              " '99a76ddc-2352-42cf-abd8-624d38e60a0d',\n",
              " 'ff8ad444-77bb-42a6-9df1-82e1f638c38d',\n",
              " 'bac5c206-4abc-45ee-aedf-bd28b0be98f7',\n",
              " '3d972d51-a226-45d9-807b-80850f5b4655']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Retrival\n",
        "\n",
        "**`similarity_search()`**"
      ],
      "metadata": {
        "id": "m87uzEY-adKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=3,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CudVH0Ola6hu",
        "outputId": "abdd90d5-bea0-4ad3-9d9f-78ff87d2ac95"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`similarity_search_with_score()`**"
      ],
      "metadata": {
        "id": "mo7X-0SodiK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\"\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmfBWpyjdjuZ",
        "outputId": "4778d63d-64fd-494f-b596-9adb58e8f52e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.668031] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n",
            "* [SIM=0.667716] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n",
            "* [SIM=0.577411] I have a bad feeling I am going to get deleted :( [{'source': 'tweet'}]\n",
            "* [SIM=0.577374] I have a bad feeling I am going to get deleted :( [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\", k=2, filter={\"source\": \"news\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OifPcNlPeJlZ",
        "outputId": "81f1a810-89e8-4def-a0db-247c74b3922b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.668031] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n",
            "* [SIM=0.668031] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
          ]
        }
      ]
    }
  ]
}